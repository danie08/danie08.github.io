
@article{occhipinti:etal:2020italianlp,
  title={ItaliaNLP @ TAG-IT: UmBERTo for Author Profiling at TAG-it 2020},
  author={Occhipinti, Daniela and Tesei, Andrea and Iacono, Maria and Aliprandi, Carlo and De Mattei, Lorenzo},
  journal={EVALITA Evaluation of NLP and Speech Tools for Italian-December 17th, 2020},
  pages={263},
  year={2020},
  url={https://ceur-ws.org/Vol-2765/paper143.pdf}
}

@inproceedings{occhipinti:etal-2024-prodigy,
    title = "{PRODIG}y: a {PRO}file-based {DI}alogue Generation dataset",
    author = "Occhipinti, Daniela  and
      Tekiro{\u{g}}lu, Serra Sinem  and
      Guerini, Marco",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.222",
    doi = "10.18653/v1/2024.findings-naacl.222",
    pages = "3500--3514",
    abstract = "Providing dialogue agents with a profile representation can improve their consistency and coherence, leading to better conversations. However, current profile-based dialogue datasets for training such agents contain either explicit profile representations that are simple and dialogue-specific, or implicit representations that are difficult to collect. In this work, we introduce the PRODIGy (PROfile-based DIalogue Generation) dataset, which brings diverse representations together, providing a more comprehensive profile dimension set for each speaker. This resource comprises more than 20k dialogues, sourced from movie scripts, aligned with speaker representations such as communication style, biography, personality and gender. Initial experiments with diverse baselines show that providing generative language models with these aspects of a profile, both separately and jointly, enhances models{'} performance. This improvement holds true in both in-domain and cross-domain settings, for both fine-tuned and instruction-based LLMs.",
}

@inproceedings{occhipinti-etal-2024-fine,
    title = "Fine-tuning with {HED}-{IT}: The impact of human post-editing for dialogical language models",
    author = "Occhipinti, Daniela  and
      Marchi, Michele  and
      Mondella, Irene  and
      Lai, Huiyuan  and
      Dell{'}Orletta, Felice  and
      Nissim, Malvina  and
      Guerini, Marco",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.707",
    doi = "10.18653/v1/2024.findings-acl.707",
    pages = "11892--11907",
    abstract = "Automatic methods for generating and gathering linguistic data have proven effective for fine-tuning Language Models (LMs) in languages less resourced than English. Still, while there has been emphasis on data quantity, less attention has been given to its quality. In this work, we investigate the impact of human intervention on machine-generated data when fine-tuning dialogical models. In particular, we study (1) whether post-edited dialogues exhibit higher perceived quality compared to the originals that were automatically generated; (2) whether fine-tuning with post-edited dialogues results in noticeable differences in the generated outputs; and (3) whether post-edited dialogues influence the outcomes when considering the parameter size of the LMs. To this end we created HED-IT, a large-scale dataset where machine-generated dialogues are paired with the version post-edited by humans. Using both the edited and unedited portions of HED-IT, we fine-tuned three different sizes of an LM. Results from both human and automatic evaluation show that the different quality of training data is clearly perceived and it has an impact also on the models trained on such data. Additionally, our findings indicate that larger models are less sensitive to data quality, whereas this has a crucial impact on smaller models. These results enhance our comprehension of the impact of human intervention on training data in the development of high-quality LMs.",
}

